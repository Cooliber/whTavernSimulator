#!/usr/bin/env python3
"""
Direct LLM test to verify API connections
"""

import os
from dotenv import load_dotenv
from crewai.llm import LLM
import requests
import json

# Load environment variables
load_dotenv()

def test_groq_llm():
    """Test Groq LLM directly"""
    print("ğŸ§ª Testing Groq LLM...")
    
    groq_key = os.getenv("groq_api_key")
    if not groq_key:
        print("âŒ Groq API key not found")
        return False
    
    try:
        # Try the groq/ prefix approach first
        llm = LLM(
            model="groq/llama3-70b-8192",
            api_key=groq_key
        )
        
        # Test simple call
        response = llm.call([{"role": "user", "content": "Say hello from Karl Franz, Emperor of the Empire!"}])
        # Handle both string and object responses
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        print(f"âœ… Groq response: {content[:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ Groq LLM failed: {e}")
        print(f"   Error type: {type(e).__name__}")
        import traceback
        print(f"   Full traceback:")
        traceback.print_exc()
        return False

def test_cerebras_llm():
    """Test Cerebras LLM directly"""
    print("\nğŸ§ª Testing Cerebras LLM...")
    
    cerebras_key = os.getenv("Cerebras_api")
    if not cerebras_key:
        print("âŒ Cerebras API key not found")
        return False
    
    try:
        # Try different approaches for Cerebras
        # First try: use openai/ prefix with custom base_url
        llm = LLM(
            model="openai/llama3.1-8b",
            api_key=cerebras_key,
            base_url="https://api.cerebras.ai/v1"
        )
        
        # Test simple call
        response = llm.call([{"role": "user", "content": "Say hello from a Witch Hunter!"}])
        # Handle both string and object responses
        if hasattr(response, 'content'):
            content = response.content
        else:
            content = str(response)
        print(f"âœ… Cerebras response: {content[:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ Cerebras LLM failed: {e}")
        print(f"   Error type: {type(e).__name__}")
        import traceback
        print(f"   Full traceback:")
        traceback.print_exc()
        return False

def test_groq_direct():
    """Test Groq API directly with requests"""
    print("\nğŸ§ª Testing Groq API directly with requests...")

    groq_key = os.getenv("groq_api_key")
    if not groq_key:
        print("âŒ Groq API key not found")
        return False

    try:
        headers = {
            "Authorization": f"Bearer {groq_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": "llama3-70b-8192",
            "messages": [
                {"role": "user", "content": "Say hello from Karl Franz, Emperor of the Empire!"}
            ],
            "max_tokens": 100
        }

        response = requests.post(
            "https://api.groq.com/openai/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            print(f"âœ… Groq direct response: {content[:100]}...")
            return True
        else:
            print(f"âŒ Groq direct failed: {response.status_code} - {response.text}")
            return False

    except Exception as e:
        print(f"âŒ Groq direct failed: {e}")
        return False

def test_cerebras_direct():
    """Test Cerebras API directly with requests"""
    print("\nğŸ§ª Testing Cerebras API directly with requests...")

    cerebras_key = os.getenv("Cerebras_api")
    if not cerebras_key:
        print("âŒ Cerebras API key not found")
        return False

    try:
        headers = {
            "Authorization": f"Bearer {cerebras_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": "llama3.1-8b",
            "messages": [
                {"role": "user", "content": "Say hello from a Witch Hunter!"}
            ],
            "max_tokens": 100
        }

        response = requests.post(
            "https://api.cerebras.ai/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            content = data["choices"][0]["message"]["content"]
            print(f"âœ… Cerebras direct response: {content[:100]}...")
            return True
        else:
            print(f"âŒ Cerebras direct failed: {response.status_code} - {response.text}")
            return False

    except Exception as e:
        print(f"âŒ Cerebras direct failed: {e}")
        return False

def main():
    """Main test function"""
    print("ğŸ”‘ API Key Direct LLM Test")
    print("=" * 40)

    # Test CrewAI LLM wrapper
    groq_success = test_groq_llm()
    cerebras_success = test_cerebras_llm()

    # Test direct API calls
    groq_direct_success = test_groq_direct()
    cerebras_direct_success = test_cerebras_direct()

    print("\n" + "=" * 40)
    print("ğŸ“Š RESULTS:")
    print(f"  Groq LLM (CrewAI): {'âœ…' if groq_success else 'âŒ'}")
    print(f"  Cerebras LLM (CrewAI): {'âœ…' if cerebras_success else 'âŒ'}")
    print(f"  Groq API (Direct): {'âœ…' if groq_direct_success else 'âŒ'}")
    print(f"  Cerebras API (Direct): {'âœ…' if cerebras_direct_success else 'âŒ'}")

    if groq_direct_success and cerebras_direct_success:
        print("\nğŸ‰ Both APIs working directly! Issue is with CrewAI LLM wrapper.")
    elif groq_success and cerebras_success:
        print("\nğŸ‰ Both LLMs working! API keys are correctly configured.")
    else:
        print("\nâš ï¸ Some APIs failed. Check API keys and network connection.")

if __name__ == "__main__":
    main()
